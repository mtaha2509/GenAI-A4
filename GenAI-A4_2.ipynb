{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27dc84dc-2eb4-479c-a5a9-1379379cdf72",
   "metadata": {},
   "source": [
    "### PART-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2b9e33-d8dd-42b4-b712-6ae9c7269af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4000, 'validation': 500, 'test': 500}\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if needed\n",
    "# pip install datasets transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import PegasusTokenizer\n",
    "\n",
    "# 1. Load & subsample 5 000 examples\n",
    "raw = load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
    "raw = raw.shuffle(seed=42).select(range(5000))\n",
    "\n",
    "# 2. Load Pegasus‐XSum tokenizer and set special tokens\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "# By default PegasusTokenizer.pad_token == '<pad>' and eos_token == '</s>'\n",
    "# If you want to pad with eos instead of a separate pad symbol:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 3. Define max lengths\n",
    "max_input_length  = 512   # Pegasus‐XSum max supported input\n",
    "max_target_length = 256\n",
    "\n",
    "# 4. Tokenization function\n",
    "def tokenize_fn(batch):\n",
    "    # Encode articles\n",
    "    enc = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    # Encode abstracts as labels\n",
    "    dec = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_target_length,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": dec[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "# 5. Apply tokenization and remove original columns\n",
    "tokenized = raw.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=raw.column_names,\n",
    ")\n",
    "\n",
    "# 6. Split into Train/Validation/Test (80/10/10)\n",
    "split_1 = tokenized.train_test_split(test_size=0.20, seed=42)\n",
    "split_2 = split_1[\"test\"].train_test_split(test_size=0.50, seed=42)\n",
    "\n",
    "datasets = {\n",
    "    \"train\": split_1[\"train\"],         # 4 000 samples\n",
    "    \"validation\": split_2[\"train\"],    #   500 samples\n",
    "    \"test\": split_2[\"test\"],           #   500 samples\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in datasets.items()})\n",
    "# -> {'train': 4000, 'validation': 500, 'test': 500}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93cc29c-053d-4655-a09b-ddd189b6986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Reload & split raw test set (to restore article/abstract)\n",
    "raw = (\n",
    "    load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(5000))\n",
    ")\n",
    "split1 = raw.train_test_split(test_size=0.20, seed=42)\n",
    "split2 = split1[\"test\"].train_test_split(test_size=0.50, seed=42)\n",
    "raw_test = split2[\"test\"]  # has 'article' & 'abstract'\n",
    "\n",
    "# 2. Reattach text columns to your tokenized-only test split\n",
    "#    (assumes `datasets[\"test\"]` exists from your earlier tokenization)\n",
    "tokenized_test = (\n",
    "    datasets[\"test\"]\n",
    "    .add_column(\"article\", raw_test[\"article\"])\n",
    "    .add_column(\"abstract\", raw_test[\"abstract\"])\n",
    ")\n",
    "\n",
    "# 3. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "# 4. Load two separate model instances:\n",
    "#    A. Base Pegasus\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/pegasus-xsum\", device_map=\"auto\"\n",
    ")\n",
    "#    B. Fine-tuned (wraps a fresh base internally)\n",
    "ft_base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/pegasus-xsum\", device_map=\"auto\"\n",
    ")\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    ft_base_model, \"./lora-pegasus-xsum\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 5. Summarization helper\n",
    "def generate_summaries(model, articles):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for art in articles:\n",
    "        inputs = tokenizer(\n",
    "            art,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        sum_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        outs.append(tokenizer.decode(sum_ids[0], skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "# 6. Select 10 samples and run inference\n",
    "articles      = tokenized_test[\"article\"][:10]\n",
    "ground_truths = tokenized_test[\"abstract\"][:10]\n",
    "\n",
    "base_summaries      = generate_summaries(base_model,      articles)\n",
    "finetuned_summaries = generate_summaries(finetuned_model, articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63d71e1-21fa-40ee-90a2-365fd707167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting together\n",
      "  Downloading together-1.5.8-py3-none-any.whl (88 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 KB\u001b[0m \u001b[31m484.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.9.3 in ./.local/lib/python3.10/site-packages (from together) (3.10.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in ./.local/lib/python3.10/site-packages (from together) (2.32.3)\n",
      "Collecting rich<15.0.0,>=13.8.1\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.2 in ./.local/lib/python3.10/site-packages (from together) (4.67.1)\n",
      "Collecting pillow<12.0.0,>=11.1.0\n",
      "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in ./.local/lib/python3.10/site-packages (from together) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in ./.local/lib/python3.10/site-packages (from together) (2.10.6)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in ./.local/lib/python3.10/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in ./.local/lib/python3.10/site-packages (from together) (0.12.5)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in ./.local/lib/python3.10/site-packages (from together) (3.14.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in ./.local/lib/python3.10/site-packages (from together) (8.1.8)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in ./.local/lib/python3.10/site-packages (from together) (19.0.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in ./.local/lib/python3.10/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.16.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (4.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->together) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.31.0->together) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->together) (2.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.10/site-packages (from rich<15.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.10/site-packages (from rich<15.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.local/lib/python3.10/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->together) (0.1.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.9.3->together) (0.2.0)\n",
      "Installing collected packages: pillow, rich, together\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.4.2\n",
      "    Uninstalling rich-13.4.2:\n",
      "      Successfully uninstalled rich-13.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.42.0 requires rich<14,>=10.14.0, but you have rich 14.0.0 which is incompatible.\n",
      "openxlab 0.1.2 requires requests~=2.28.2, but you have requests 2.32.3 which is incompatible.\n",
      "openxlab 0.1.2 requires rich~=13.4.2, but you have rich 14.0.0 which is incompatible.\n",
      "openxlab 0.1.2 requires tqdm~=4.65.0, but you have tqdm 4.67.1 which is incompatible.\n",
      "gradio 4.43.0 requires markupsafe~=2.0, but you have markupsafe 3.0.2 which is incompatible.\n",
      "gradio 4.43.0 requires pillow<11.0,>=8.0, but you have pillow 11.2.1 which is incompatible.\n",
      "controlnet-aux 0.0.9 requires timm<=0.6.7, but you have timm 1.0.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pillow-11.2.1 rich-14.0.0 together-1.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79ce1ff-a167-4ec9-b581-41954024a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOGETHER_API_KEY=6f2d3698eb38ec3c208297ffca14741a07fc8b07295dd557c37fbf9656865db8\n"
     ]
    }
   ],
   "source": [
    "%env TOGETHER_API_KEY=6f2d3698eb38ec3c208297ffca14741a07fc8b07295dd557c37fbf9656865db8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1816493a-d84a-47ed-bcc2-435a006257ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()  # reads TOGETHER_API_KEY automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a767db18-56cf-4c68-9475-968cc402618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data: replace these lists with your actual data\n",
    "originals           = articles           # list of 10 original texts\n",
    "finetuned_summaries = finetuned_summaries  # list of 10 fine-tuned summaries\n",
    "base_summaries      = base_summaries       # list of 10 base-model summaries\n",
    "\n",
    "# Choose which summaries to evaluate here:\n",
    "to_eval = list(zip(originals, finetuned_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9f285a-38ff-4cd3-be94-983ad0612251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c7f44e4-75e9-4d59-81ef-b40c09a1bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 → {'fluency': 4, 'factuality': 5, 'coverage': 2}\n",
      "Sample 1 → {'fluency': 3, 'factuality': 4, 'coverage': 2}\n",
      "Sample 2 → {'fluency': 1, 'factuality': 1, 'coverage': 1}\n",
      "Sample 3 → {'fluency': 4, 'factuality': 5, 'coverage': 2}\n",
      "Sample 4 → {'fluency': 4, 'factuality': 5, 'coverage': 2}\n",
      "Sample 5 → {'fluency': 1, 'factuality': 2, 'coverage': 1}\n",
      "Sample 6 → {'fluency': 1, 'factuality': 1, 'coverage': 1}\n",
      "Sample 7 → {'fluency': 2, 'factuality': 3, 'coverage': 1}\n",
      "Sample 8 → {'fluency': 5, 'factuality': 4, 'coverage': 2}\n",
      "Sample 9 → {'fluency': 4, 'factuality': 2, 'coverage': 2}\n",
      "Average Judge Scores: {'fluency': 2.9, 'factuality': 3.2, 'coverage': 1.6}\n"
     ]
    }
   ],
   "source": [
    "def generate_summaries(model, articles):\n",
    "    outs = []\n",
    "    for art in articles:\n",
    "        inputs = tokenizer(art, truncation=True, padding=\"longest\", max_length=512, return_tensors=\"pt\").to(model.device)\n",
    "        sum_ids = model.generate(**inputs, max_length=256, num_beams=4, early_stopping=True)\n",
    "        outs.append(tokenizer.decode(sum_ids[0], skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "# Run inference on 10 samples\n",
    "articles = tokenized_test[\"article\"][:10]\n",
    "base_summaries = generate_summaries(base_model, articles)\n",
    "finetuned_summaries = generate_summaries(finetuned_model, articles)\n",
    "\n",
    "# Evaluation helpers\n",
    "def truncate_to_tokens(text: str, max_chars: int = 2000) -> str:\n",
    "    return text if len(text) <= max_chars else text[:max_chars] + \"…\"\n",
    "\n",
    "def extract_json(content: str) -> dict:\n",
    "    fence = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", content, flags=re.DOTALL)\n",
    "    if fence:\n",
    "        payload = fence.group(1)\n",
    "    else:\n",
    "        brace = re.search(r\"\\{.*\\}\", content, flags=re.DOTALL)\n",
    "        if not brace:\n",
    "            raise ValueError(\"No JSON found\")\n",
    "        payload = brace.group(0)\n",
    "    return json.loads(payload)\n",
    "\n",
    "def evaluate_with_llm(orig: str, summ: str, model_name=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\") -> dict:\n",
    "    orig_trunc = truncate_to_tokens(orig, 2000)\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator. Rate the following summary on Fluency, Factuality, and Coverage (1–5).\n",
    "\n",
    "Original Text (truncated):\n",
    "{orig_trunc}\n",
    "\n",
    "Generated Summary:\n",
    "{summ}\n",
    "\n",
    "Respond only with a JSON object inside a markdown code fence labelled json:\n",
    "```json\n",
    "{{\n",
    "  \"fluency\": <int>,\n",
    "  \"fluency_justification\": \"<brief>\",\n",
    "  \"factuality\": <int>,\n",
    "  \"factuality_justification\": \"<brief>\",\n",
    "  \"coverage\": <int>,\n",
    "  \"coverage_justification\": \"<brief>\"\n",
    "}}\n",
    "``` \"\"\"\n",
    "    resp = client.chat.completions.create(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    if not text:\n",
    "        time.sleep(1)\n",
    "        resp = client.chat.completions.create(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "    return extract_json(text)\n",
    "\n",
    "# Run evaluation on 10 samples\n",
    "records = []\n",
    "for i in range(10):\n",
    "    orig = tokenized_test[\"article\"][i]\n",
    "    summ = finetuned_summaries[i]\n",
    "    try:\n",
    "        scores = evaluate_with_llm(orig, summ)\n",
    "    except Exception as e:\n",
    "        print(f\"Sample {i} failed:\", e)\n",
    "        continue\n",
    "    scores[\"index\"] = i\n",
    "    records.append(scores)\n",
    "    print(f\"Sample {i} →\", {k: scores[k] for k in (\"fluency\",\"factuality\",\"coverage\")})\n",
    "    time.sleep(1)\n",
    "\n",
    "# Aggregate results\n",
    "df = pd.DataFrame(records)\n",
    "avg_scores = df[[\"fluency\",\"factuality\",\"coverage\"]].mean()\n",
    "print(\"Average Judge Scores:\", avg_scores.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a742c6e2-62bc-48ee-880f-565b7a8692ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 0 ---\n",
      "Fluency:    4\n",
      "  ↳ The summary is well-written and easy to understand, but it contains a repeated sentence which makes it slightly less fluent.\n",
      "Factuality: 5\n",
      "  ↳ The summary accurately represents the main ideas of the original text without introducing any errors or inaccuracies.\n",
      "Coverage:   2\n",
      "  ↳ The summary only covers a small portion of the original text, missing important details about the scheduling problems and machine environments.\n",
      "\n",
      "--- Sample 1 ---\n",
      "Fluency:    3\n",
      "  ↳ The summary is mostly coherent, but it contains repetitive sentences and lacks a clear structure, making it somewhat difficult to follow.\n",
      "Factuality: 4\n",
      "  ↳ The summary accurately reports on the detection of the acceleration of gravity and the experimental approaches used, but it omits some details and context from the original text.\n",
      "Coverage:   2\n",
      "  ↳ The summary only covers a limited portion of the original text, focusing on the detection of the acceleration of gravity and the experimental approaches, while omitting the broader context and other relevant information.\n",
      "\n",
      "--- Sample 2 ---\n",
      "Fluency:    1\n",
      "  ↳ The summary is incoherent and repetitive, making it difficult to understand.\n",
      "Factuality: 1\n",
      "  ↳ The summary does not accurately represent the original text and introduces nonsensical phrases.\n",
      "Coverage:   1\n",
      "  ↳ The summary fails to capture the main points of the original text, including the effects of spin-orbit coupling on transport in a two-dimensional electron gas.\n",
      "\n",
      "--- Sample 3 ---\n",
      "Fluency:    4\n",
      "  ↳ The summary is well-structured and easy to follow, but it contains some technical terms and jargon that may be unfamiliar to non-experts.\n",
      "Factuality: 5\n",
      "  ↳ The summary accurately represents the main points of the original text, including the use of the TRBV3 platform and the novel approach to continuous data recording.\n",
      "Coverage:   2\n",
      "  ↳ The summary only covers a small portion of the original text, omitting details about the J-PET prototype, its applications, and the detector readout chain.\n",
      "\n",
      "--- Sample 4 ---\n",
      "Fluency:    4\n",
      "  ↳ The summary is well-structured and easy to understand, but it is a direct copy of a sentence from the original text, which may not be ideal for a summary.\n",
      "Factuality: 5\n",
      "  ↳ The summary accurately represents the original text and does not introduce any factual errors.\n",
      "Coverage:   2\n",
      "  ↳ The summary only covers a small part of the original text and does not provide a comprehensive overview of the main topics discussed.\n",
      "\n",
      "--- Sample 5 ---\n",
      "Fluency:    1\n",
      "  ↳ The summary is repetitive and lacks coherence, making it difficult to read and understand.\n",
      "Factuality: 2\n",
      "  ↳ The summary mentions a few facts from the original text, but it does not provide any meaningful information and is mostly repetitive.\n",
      "Coverage:   1\n",
      "  ↳ The summary only covers a very small aspect of the original text and fails to capture the main points and ideas discussed.\n",
      "\n",
      "--- Sample 6 ---\n",
      "Fluency:    1\n",
      "  ↳ The generated summary is incoherent and repetitive, with multiple identical sentences that do not form a logical narrative.\n",
      "Factuality: 1\n",
      "  ↳ The summary does not accurately represent the original text, and the repetitive sentences do not convey any meaningful information.\n",
      "Coverage:   1\n",
      "  ↳ The summary fails to capture the main points of the original text, including the importance of tensor decomposition, generic identifiability, and the relationship between multilinear algebra and projective geometry.\n",
      "\n",
      "--- Sample 7 ---\n",
      "Fluency:    2\n",
      "  ↳ The summary is repetitive and lacks coherence, making it difficult to read and understand.\n",
      "Factuality: 3\n",
      "  ↳ The summary accurately conveys the main topic of the original text, but it does not provide any additional information or context.\n",
      "Coverage:   1\n",
      "  ↳ The summary only covers the first sentence of the original text and does not provide any information about the rest of the content.\n",
      "\n",
      "--- Sample 8 ---\n",
      "Fluency:    5\n",
      "  ↳ The summary is well-written, clear, and concise, with proper grammar and sentence structure.\n",
      "Factuality: 4\n",
      "  ↳ The summary accurately conveys the main idea of the original text, but omits some details and supporting evidence.\n",
      "Coverage:   2\n",
      "  ↳ The summary only captures a small portion of the original text, missing key points about the importance of photometry, colour-magnitude diagrams, and the benefits of high spatial resolution.\n",
      "\n",
      "--- Sample 9 ---\n",
      "Fluency:    4\n",
      "  ↳ The summary is well-structured and easy to understand, but it lacks a clear connection between the three axes of galaxy evolution and the types of galaxies mentioned.\n",
      "Factuality: 3\n",
      "  ↳ The summary inaccurately represents the original text by introducing 'blue early-type galaxies' which is not mentioned in the original text. The original text only mentions 'red early-type galaxies' and 'blue late-type galaxies'.\n",
      "Coverage:   2\n",
      "  ↳ The summary only covers a small portion of the original text, missing key points such as the complexity of galaxy properties, the importance of environment and mass, and the previous studies on galaxy evolution.\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for i in range(10):\n",
    "    orig = tokenized_test[\"article\"][i]\n",
    "    summ = finetuned_summaries[i]\n",
    "    try:\n",
    "        scores = evaluate_with_llm(orig, summ)\n",
    "    except Exception as e:\n",
    "        print(f\"Sample {i} failed:\", e)\n",
    "        continue\n",
    "\n",
    "    # Keep both scores and justifications\n",
    "    record = {\n",
    "        \"index\": i,\n",
    "        \"fluency\": scores[\"fluency\"],\n",
    "        \"fluency_justification\": scores[\"fluency_justification\"],\n",
    "        \"factuality\": scores[\"factuality\"],\n",
    "        \"factuality_justification\": scores[\"factuality_justification\"],\n",
    "        \"coverage\": scores[\"coverage\"],\n",
    "        \"coverage_justification\": scores[\"coverage_justification\"],\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "    # Print everything\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Fluency:    {record['fluency']}\")\n",
    "    print(f\"  ↳ {record['fluency_justification']}\")\n",
    "    print(f\"Factuality: {record['factuality']}\")\n",
    "    print(f\"  ↳ {record['factuality_justification']}\")\n",
    "    print(f\"Coverage:   {record['coverage']}\")\n",
    "    print(f\"  ↳ {record['coverage_justification']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e27961-3b0b-41ca-83b4-0c98bae4d8b2",
   "metadata": {},
   "source": [
    "## PART-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f7b5cca-9879-479f-9600-9017135e17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import requests\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from transformers import PegasusTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63fdf382-6de5-400d-aa99-97c2febc5b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-16 08:29:58,573 - __main__ - INFO - Keywords raw: [\"Explainable AI\", \"Artificial Intelligence Explainability\", \"Model Interpretability\", \"Machine Learning Transparency\", \"Deep Learning Explainability\", \"Neural Network Interpretability\", \"Feature Importance\", \"Partial Dependence Plots\", \"SHAP Values\", \"LIME\", \"Tree Explainer\", \"Model-Agnostic Interpretability\"]\n",
      "2025-05-16 08:29:59,207 - __main__ - WARNING - xmltodict failed: unclosed token: line 183, column 4; trying lxml recovery\n",
      "2025-05-16 08:29:59,208 - __main__ - ERROR - lxml fallback failed: 'str' object has no attribute 'get'\n",
      "2025-05-16 08:29:59,907 - __main__ - WARNING - xmltodict failed: unclosed token: line 169, column 4; trying lxml recovery\n",
      "2025-05-16 08:29:59,910 - __main__ - ERROR - lxml fallback failed: 'str' object has no attribute 'get'\n",
      "2025-05-16 08:30:01,046 - __main__ - WARNING - xmltodict failed: unclosed token: line 173, column 4; trying lxml recovery\n",
      "2025-05-16 08:30:02,025 - __main__ - WARNING - xmltodict failed: unclosed token: line 187, column 4; trying lxml recovery\n",
      "2025-05-16 08:30:02,027 - __main__ - ERROR - lxml fallback failed: 'str' object has no attribute 'get'\n",
      "2025-05-16 08:30:02,942 - __main__ - WARNING - xmltodict failed: no element found: line 181, column 61; trying lxml recovery\n",
      "2025-05-16 08:30:02,944 - __main__ - ERROR - lxml fallback failed: 'str' object has no attribute 'get'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Agent Report ===\n",
      "[\n",
      "  {\n",
      "    \"title\": \"The Definitions of Interpretability and Learning of Interpretable Models\",\n",
      "    \"authors\": [\n",
      "      \"Weishen Pan\",\n",
      "      \"Changshui Zhang\"\n",
      "    ],\n",
      "    \"year\": 2021,\n",
      "    \"summary\": \"We propose a mathematical definition for the human-interpretable model, which can provide an entire decision-making process that is human-understandable. Experiments on image datasets show the advantages of our proposed model in two aspects: 1) The completely human-interpretable model can provide an entire decision-making process that is human-understandable; 2) The completely human-interpretable model is more robust against adversarial attacks.\",\n",
      "    \"methodology\": \"\",\n",
      "    \"contributions\": \"we propose a mathematical definition for the human-interpretable model.\",\n",
      "    \"limitations\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Bi-interpretation in weak set theories\",\n",
      "    \"authors\": [\n",
      "      \"Alfredo Roque Freire\",\n",
      "      \"Joel David Hamkins\"\n",
      "    ],\n",
      "    \"year\": 2020,\n",
      "    \"summary\": \"In this paper, we study the bi-interpretable nature of natural weaker set theories, including Zermelo-Fraenkel set theory $textZFC-$ without power set and Zermelo set theory Z.\",\n",
      "    \"methodology\": \"\",\n",
      "    \"contributions\": \"\",\n",
      "    \"limitations\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"iCapsNets: Towards Interpretable Capsule Networks for Text\\n  Classification\",\n",
      "    \"authors\": [\n",
      "      \"Zhengyang Wang\",\n",
      "      \"Xia Hu\",\n",
      "      \"Shuiwang Ji\"\n",
      "    ],\n",
      "    \"year\": 2020,\n",
      "    \"summary\": \"In this work, we propose interpretable deep learning models that are easy to interpret and achieve global interpretability.\",\n",
      "    \"methodology\": \" are easy to interpret but have low accuracies. The development of deep\\nlearning models boosts the performance significantly. However, deep learning\\nmodels are typically hard to interpret. In this work, we propose interpretable\\ncapsule networks (iCapsNets) to bridge this gap. iCapsNets use capsules to\\nmodel semantic meanings and explore novel methods to increase interpretability.\\nThe design of iCapsNets is consistent with human intuition and enables it to\\nproduce human-understandable interpretation results. Notably, iCapsNets can be\\ninterpreted both locally and globally. In terms of local interpretability,\\niCapsNets offer a simple yet effective method to explain the predictions for\\neach data sample. On the other hand, iCapsNets explore a novel way to explain\\nthe model's general behavior, achieving global interpretability. Experimental\\nstudies show that our iCapsNets yield meaningful local and global\\ninterpretation results, without suffering from significant performance loss\\ncompared to non-interpretable methods.\",\n",
      "    \"contributions\": \"we propose interpretable\\ncapsule networks (iCapsNets) to bridge this gap.\",\n",
      "    \"limitations\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Hybrid Predictive Model: When an Interpretable Model Collaborates with a\\n  Black-box Model\",\n",
      "    \"authors\": [\n",
      "      \"Tong Wang\",\n",
      "      \"Qihang Lin\"\n",
      "    ],\n",
      "    \"year\": 2019,\n",
      "    \"summary\": \"We propose a novel framework for building a Hybrid Predictive Model (HPM) that integrates an interpretable model with any black-box model to combine their strengths.\",\n",
      "    \"methodology\": \"\",\n",
      "    \"contributions\": \"we propose a novel\\nframework for building a Hybrid Predictive Model (HPM) that integrates an\\ninterpretable model with any black-box model to combine their strengths.\",\n",
      "    \"limitations\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Machine Learning Model Interpretability for Precision Medicine\",\n",
      "    \"authors\": [\n",
      "      \"Gajendra Jung Katuwal\",\n",
      "      \"Robert Chen\"\n",
      "    ],\n",
      "    \"year\": 2016,\n",
      "    \"summary\": \"In this paper, we show that complex models such as random forest can be made interpretable.\",\n",
      "    \"methodology\": \"\",\n",
      "    \"contributions\": \"\",\n",
      "    \"limitations\": \"\"\n",
      "  }\n",
      "]\n",
      "=== Direct Pegasus Summaries (10 samples) ===\n",
      "Sample 1:\n",
      "Base Model:    We study the robustness of local search and greedy algorithms for scheduling problems .\n",
      "Fine-tuned:    we study the performance guarantee of local search and greedy algorithms for scheduling problems in the framework of smoothed analysis , in which instances are subject to some degree of random noise . since pure nash equilibria can be seen as local optima , our results also imply a new bound on the smoothed price of anarchy , showing that known worst - case results are too pessimistic in the presence of noise . since pure nash equilibria can be seen as local optima , our results also imply a new bound on the smoothed price of anarchy , showing that known worst - case results are too pessimistic in the presence of noise . since pure nash equilibria can be seen as local optima , our results also imply a new bound on the smoothed price of anarchy , showing that known worst - case results are too pessimistic in the presence of noise . since pure nash equilibria can be seen as local optima , our results also imply a new bound on the smoothed price of anarchy , showing that known worst - case results are too pessimistic in the presence of noise .\n",
      "Sample 2:\n",
      "Base Model:    In our group we have developed a novel method for the detection of falling atomic pulses induced by the acceleration of gravity.\n",
      "Fine-tuned:    we report the detection of the acceleration of gravity @xmath0 in the falling atomic cloud . the detection is performed by imaging the falling atomic cloud using absorption imaging techniques and the uncertainty on the @xmath0 determination did not exceed @xmath7 dominated by the imaging system . in both experiments the detection is performed by imaging the falling atomic cloud using absorption imaging techniques and the uncertainty on the @xmath0 determination did not exceed @xmath7 dominated by the imaging system . our experimental approach is based on the precise determination of the velocity distribution of atoms along the vertical axis using doppler - sensitive raman transitions . this is performed by applying a @xmath1 pulses sequence with a spacing time @xmath8 : the first pulse defines an initial velocity by selecting a narrow velocity class from an ultracold atomic sample . the second pulse measures the final velocity distribution of the atoms after the fall by transferring a resonant velocity slice to another internal state .\n",
      "Sample 3:\n",
      "Base Model:    The spin - orbit coupling of a two dimensional electron gas is affected by the presence or absence of a time reversal symmetry breaking magnetic field.\n",
      "Fine-tuned:    we study the dependence of the weak correction to the spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin -\n",
      "Sample 4:\n",
      "Base Model:    scintillators are used in a wide range of scientific experiments .\n",
      "Fine-tuned:    we present a complete solution for the continuous data recording ( trigger - less ) of the j - pet daq system , based on trbv3 ( trbv3 ) platform @xcite , developed for and widely used in high energy physics experiments @xcite . our solution for the continuous data recording ( trigger - less ) is a novel approach in such detector systems and assures that most of the information is preserved on the storage for further , high - level processing . our solution for the continuous data recording ( trigger - less ) is a novel approach in such detector systems and assures that most of the information is preserved on the storage for further , high - level processing . our solution for the continuous data recording ( trigger - less ) is a novel approach in such detector systems and assures that most of the information is preserved on the storage for further , high - level processing . our solution for the continuous data recording ( trigger - less ) is a novel approach in such detector systems and assures that most of the information is preserved on the storage for further , high - level processing . our solution for the continuous data recording ( trigger - less ) is a novel approach in such detector systems and\n",
      "Sample 5:\n",
      "Base Model:    here we report a new study on synaptic plasticity in the brain.\n",
      "Fine-tuned:    we show that an important plasticity mechanism , spike timing dependent plasticity , can lead to autonomous emergence of these large scale structures in the brain : in contrast to previous theoretical proposals , we show that the emergence can occur autonomously even if instructive signals are not fed into the neural network while its form is shaped by synaptic plasticity .\n",
      "Sample 6:\n",
      "Base Model:    dynamic wetting flows @xcite.\n",
      "Fine-tuned:    in this paper , we study the dynamics of dynamic wetting flows @xcite , where the free surface meets the solid boundary at an angle @xmath0 , which is less than @xmath1 ( figure [ f : wetting]b ) . we find that the free surface meets the solid boundary at an angle @xmath0 , which is less than @xmath1 ( figure [ f : wetting]b ) . we find that the free surface meets the solid boundary at an angle @xmath0 , which is less than @xmath1 ( figure [ f : wetting]b ) . we find that the free surface meets the solid boundary at an angle @xmath0 , which is less than @xmath1 ( figure [ f : wetting]b ) . we find that the free surface meets the solid boundary at an angle @xmath0 , which is less than @xmath1 ( figure [ f : wetting]b ) . we find that the free surface meets the solid boundary at an angle @xmath0 , which is less than @xmath1 ( figure [ f : wetting]b )\n",
      "Sample 7:\n",
      "Base Model:    tensors @xmath5 as a sum of _ simple tensors ( i.e.\n",
      "Fine-tuned:    we study the uniqueness of the decomposition of tensors @xmath5 as a sum of _ simple _ tensors ( i.e. tensors of rank @xmath6 ) . we show that , with the assumption @xmath11 s , it is possible to decompose tensors @xmath12 as a sum of _ simple _ tensors ( i.e. tensors of rank @xmath11 s ) . we show that , with the assumption @xmath11 s , it is possible to decompose tensors @xmath12 as a sum of _ simple _ tensors ( i.e. tensors of rank @xmath11 s ) . we show that , with the assumption @xmath11 s , it is possible to decompose tensors @xmath12 as a sum of _ simple _ tensors ( i.e. tensors of rank @xmath11 s ) . we show that , with the assumption @xmath11 s , it is possible to decompose tensors @xmath12 as a sum of _ simple _ tensors ( i.\n",
      "Sample 8:\n",
      "Base Model:    oscillations in neutron - star and black - hole binaries have been investigated in a number of ways .\n",
      "Fine-tuned:    in this paper we are interested in possible origins of high - frequency quasi - periodic oscillations ( qpos ) in neutron - star and black - hole low - mass x - ray binaries ( lmxbs ) , and additionally in horizontal - branch qpos in luminous neutron - star lmxbs ( z sources ) . in this paper we are interested in possible origins of high - frequency quasi - periodic oscillations ( qpos ) in neutron - star and black - hole low - mass x - ray binaries ( lmxbs ) , and additionally in horizontal - branch qpos in luminous neutron - star lmxbs ( z sources ) . in this paper we are interested in possible origins of high - frequency quasi - periodic oscillations ( qpos ) in neutron - star and black - hole low - mass x - ray binaries ( lmxbs ) , and additionally in horizontal - branch qpos in luminous neutron - star lmxbs ( z sources ) . in this paper we are interested in possible origins of high - frequency quasi - periodic oscillations ( qpos ) in neutron - star and black - hole low - mass x - ray binaries ( lmxbs ) , and additionally in horizontal -\n",
      "Sample 9:\n",
      "Base Model:    The study of resolved stellar populations provides a powerful tool to follow galaxy evolution directly in terms of physical parameters such as age ( star formation history , sfh ) , chemical composition and enrichment history, initial mass function environment, and dynamical history of the system.\n",
      "Fine-tuned:    we present the results of a survey of resolved stellar populations in our local galaxy group , which will provide a uniform picture of the global star formation properties of galaxies with a wide variety of mass , metallicity , gas content etc .\n",
      "Sample 10:\n",
      "Base Model:    galaxy evolution is one of the most important topics in astronomy .\n",
      "Fine-tuned:    we present a comprehensive study of star formation activity of galaxies along the three axes of galaxy evolution : red early - type galaxies , blue late - type galaxies and blue early - type galaxies .\n"
     ]
    }
   ],
   "source": [
    "# research_agent.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "from together import Together\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# --- 0. Logging & Together.ai Client Setup ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "if not TOGETHER_API_KEY:\n",
    "    raise RuntimeError(\"Please set TOGETHER_API_KEY in your environment\")\n",
    "client = Together(api_key=TOGETHER_API_KEY)\n",
    "\n",
    "# --- 1. Load & Split ArXiv Summarization Dataset ---\n",
    "raw = load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
    "raw = raw.shuffle(seed=42).select(range(5000))\n",
    "\n",
    "# 80/10/10 split\n",
    "splits: DatasetDict = raw.train_test_split(test_size=0.20, seed=42)\n",
    "splits[\"validation\"], splits[\"test\"] = \\\n",
    "    splits[\"test\"].train_test_split(test_size=0.50, seed=42).values()\n",
    "\n",
    "# 2. Initialize Pegasus tokenizer + LoRA model for summarization\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "base_pegasus = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "peft_base = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "finetuned_pegasus = PeftModel.from_pretrained(peft_base, \"./lora-pegasus-xsum\")\n",
    "finetuned_pegasus.eval()\n",
    "\n",
    "# 3. Map tokenization function onto each split (keeping text)\n",
    "max_input_length, max_target_length = 512, 256\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    enc = pegasus_tokenizer(\n",
    "        batch[\"article\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_input_length\n",
    "    )\n",
    "    dec = pegasus_tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_target_length\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": dec[\"input_ids\"],\n",
    "        # keep text for inference\n",
    "        \"article\": batch[\"article\"],\n",
    "        \"abstract\": batch[\"abstract\"]\n",
    "    }\n",
    "\n",
    "tokenized_splits = splits.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[]  # keep all\n",
    ")\n",
    "\n",
    "# 4. Helper functions\n",
    "# part3_search_agent.py  (excerpt)\n",
    "\n",
    "import xmltodict\n",
    "from lxml import etree\n",
    "from xml.parsers.expat import ExpatError\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_arxiv_xml(xml_str: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse arXiv Atom XML, always emitting a list of dicts that include an 'abstract' key.\n",
    "    Falls back to lxml recovery if xmltodict fails on malformed XML.\n",
    "    \"\"\"\n",
    "    def _extract(entries):\n",
    "        \"\"\"Normalize an entry (dict) to our schema.\"\"\"\n",
    "        out = []\n",
    "        for e in entries:\n",
    "            paper = {\n",
    "                \"paperId\":      e.get(\"id\", \"\"),\n",
    "                \"title\":        e.get(\"title\", \"\").strip(),\n",
    "                # xmltodict gives you 'summary', rename to 'abstract'\n",
    "                \"abstract\":     e.get(\"summary\", \"\").strip(),\n",
    "                \"authors\":      [a.get(\"name\") for a in e.get(\"author\", []) or []],\n",
    "                \"year\":         int(e.get(\"published\", \"\")[:4] or 0),\n",
    "                \"url\":          e.get(\"id\", \"\")\n",
    "            }\n",
    "            out.append(paper)\n",
    "        return out\n",
    "\n",
    "    # 1) Try strict xmltodict parse\n",
    "    try:\n",
    "        data = xmltodict.parse(xml_str)\n",
    "        entries = data.get(\"feed\", {}).get(\"entry\", [])\n",
    "        if not isinstance(entries, list):\n",
    "            entries = [entries]\n",
    "        return _extract(entries)\n",
    "    except ExpatError as e:\n",
    "        logger.warning(f\"xmltodict failed: {e}; trying lxml recovery\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in xmltodict parse: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2) Fallback via lxml recovery\n",
    "    try:\n",
    "        parser     = etree.XMLParser(recover=True)\n",
    "        root       = etree.fromstring(xml_str.encode(\"utf-8\"), parser=parser)\n",
    "        clean_bytes= etree.tostring(root, encoding=\"utf-8\")\n",
    "        data2      = xmltodict.parse(clean_bytes)\n",
    "        entries2   = data2.get(\"feed\", {}).get(\"entry\", [])\n",
    "        if not isinstance(entries2, list):\n",
    "            entries2 = [entries2]\n",
    "        return _extract(entries2)\n",
    "    except Exception as e2:\n",
    "        logger.error(f\"lxml fallback failed: {e2}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def dedupe_papers(papers):\n",
    "    seen=set(); unique=[]\n",
    "    for p in papers:\n",
    "        key = p.get(\"paperId\") or p.get(\"title\")\n",
    "        if key and key not in seen:\n",
    "            seen.add(key); unique.append(p)\n",
    "    return unique\n",
    "\n",
    "# simplistic extractors\n",
    "\n",
    "def extract_section(text, section_name):\n",
    "    parts = re.split(rf\"\\n{section_name}\", text, flags=re.IGNORECASE)\n",
    "    return parts[1].split(\"\\n\\n\")[0] if len(parts)>1 else \"\"\n",
    "\n",
    "def extract_contributions(text):\n",
    "    m=re.search(r\"we (?:propose|introduce|present)[^.]+\\.\", text, re.IGNORECASE)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "def extract_limitations(text):\n",
    "    m=re.search(r\"limitations? (?:include|are)[^.]+\\.\", text, re.IGNORECASE)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# 5. LangGraph agent nodes\n",
    "\n",
    "def expand_keywords_node(state):\n",
    "    msgs=[\n",
    "        {\"role\":\"system\",\"content\":\"Respond ONLY with a JSON array of strings.\"},\n",
    "        {\"role\":\"user\",\"content\":f\"Expand keywords into 8-12 terms: {state['seed_keywords']}\"}\n",
    "    ]\n",
    "    resp=client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=msgs, temperature=0\n",
    "    )\n",
    "    raw=resp.choices[0].message.content\n",
    "    logger.info(f\"Keywords raw: {raw}\")\n",
    "    try: arr=json.loads(raw)\n",
    "    except: arr=re.findall(r\"\\\"(.*?)\\\"\", raw)\n",
    "    return {\"expanded_keywords\": arr}\n",
    "\n",
    "\n",
    "def search_papers_node(state):\n",
    "    papers=[]\n",
    "    for kw in state['expanded_keywords'][:5]:\n",
    "        r=requests.get(\"http://export.arxiv.org/api/query\", params={\"search_query\":f\"all:{kw}\",\"max_results\":10})\n",
    "        papers+=parse_arxiv_xml(r.text)\n",
    "    return {\"raw_papers\": dedupe_papers(papers)}\n",
    "\n",
    "\n",
    "def rank_papers_node(state):\n",
    "    scored = []\n",
    "    for p in state[\"raw_papers\"]:\n",
    "        # Citation count from your initial gather (0 if missing)\n",
    "        cites = p.get(\"citationCount\", 0)\n",
    "        year  = p.get(\"year\", 0)\n",
    "        # LLM‑inferred relevance: optional, 0.0 if you skip it\n",
    "        rel   = 0.0  \n",
    "        # Compose score\n",
    "        score = 0.5 * (cites / 1000.0) \\\n",
    "              + 0.5 * (year / 2025)  \n",
    "        p[\"score\"] = score\n",
    "        scored.append(p)\n",
    "    # Pick top 5\n",
    "    top = sorted(scored, key=lambda x: x[\"score\"], reverse=True)[:5]\n",
    "    return {\"top_papers\": top}\n",
    "\n",
    "def summarize_papers_node(state):\n",
    "    sums=[]\n",
    "    for p in state['top_papers']:\n",
    "        text=p['abstract']\n",
    "        inp=pegasus_tokenizer(text, truncation=True, max_length=512, return_tensors='pt').to(finetuned_pegasus.device)\n",
    "        out=finetuned_pegasus.generate(**inp, max_length=256, num_beams=4)\n",
    "        summ=pegasus_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        sums.append({\n",
    "            'title':p['title'],'authors':p['authors'],'year':p['year'],\n",
    "            'summary':summ,\n",
    "            'methodology':extract_section(text,'Methods'),\n",
    "            'contributions':extract_contributions(text),\n",
    "            'limitations':extract_limitations(text)\n",
    "        })\n",
    "    return {'paper_summaries': sums}\n",
    "\n",
    "# 6. Orchestrate agents\n",
    "graph=StateGraph(dict)\n",
    "graph.add_node('expand',expand_keywords_node)\n",
    "graph.add_node('search',search_papers_node)\n",
    "graph.add_node('rank',rank_papers_node)\n",
    "graph.add_node('summ',summarize_papers_node)\n",
    "\n",
    "graph.add_edge(START,'expand')\n",
    "graph.add_edge('expand','search')\n",
    "graph.add_edge('search','rank')\n",
    "graph.add_edge('rank','summ')\n",
    "graph.add_edge('summ',END)\n",
    "\n",
    "agent=graph.compile()\n",
    "\n",
    "# 7. Run demos\n",
    "if __name__ == '__main__':\n",
    "    # A) Multi-agent pipeline demo\n",
    "    result = agent.invoke({'seed_keywords': 'xai'})\n",
    "    print(\"=== Multi-Agent Report ===\")\n",
    "    print(json.dumps(result.get('paper_summaries', []), indent=2))\n",
    "\n",
    "    # B) Direct summarization of 10 held-out test samples\n",
    "    print(\"=== Direct Pegasus Summaries (10 samples) ===\")\n",
    "    samples = tokenized_splits['test']['article'][:10]\n",
    "    for idx, article in enumerate(samples, start=1):\n",
    "        inputs = pegasus_tokenizer(\n",
    "            article,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(base_pegasus.device)\n",
    "        base_ids = base_pegasus.generate(**inputs, max_length=256, num_beams=4)\n",
    "        finetuned_ids = finetuned_pegasus.generate(**inputs, max_length=256, num_beams=4)\n",
    "        base_summary = pegasus_tokenizer.decode(base_ids[0], skip_special_tokens=True)\n",
    "        fine_summary = pegasus_tokenizer.decode(finetuned_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Sample {idx}:\")\n",
    "        print(f\"Base Model:    {base_summary}\")\n",
    "        print(f\"Fine-tuned:    {fine_summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304879d6-fcca-44de-a653-0efcc77ce599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
