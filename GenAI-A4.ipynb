{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5cd1f6-b2be-44dd-93dd-8b84f61c46df",
   "metadata": {},
   "source": [
    "### PART-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e802fee-f473-4b92-999c-0765d82c801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# 1. Load the full arXiv summarization dataset\n",
    "raw = load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
    "\n",
    "# 2. Subsample 5,000 examples (shuffle first for randomness)\n",
    "raw = raw.shuffle(seed=42).select(range(5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61eaf67b-1342-4700-8c02-f8b7ed2429db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has fields 'article' and 'abstract'\n",
    "inputs  = raw[\"article\"]\n",
    "targets = raw[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9515a28b-6230-4198-bb4c-45e68c1469e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559f8369a45e48bab78da4968b440a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Replace with your TinyLlama tokenizer checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def preprocess_causal(batch):\n",
    "    inputs = []\n",
    "    for article, summary in zip(batch[\"article\"], batch[\"abstract\"]):\n",
    "        # concat with EOS tokens\n",
    "        text = article + tokenizer.eos_token + summary + tokenizer.eos_token\n",
    "        ids  = tokenizer(text, truncation=True, max_length=1024)[\"input_ids\"]\n",
    "        inputs.append(ids)\n",
    "    # pad to max length and create labels = input_ids\n",
    "    batch_out = tokenizer.pad(\n",
    "        {\"input_ids\": inputs},\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_out[\"labels\"] = batch_out[\"input_ids\"].clone()\n",
    "    return batch_out\n",
    "\n",
    "tokenized = raw.map(\n",
    "    preprocess_causal,\n",
    "    batched=True,\n",
    "    remove_columns=raw.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "072ea191-e9f4-4688-a3cd-4fcebf6453a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4000, 'validation': 500, 'test': 500}\n"
     ]
    }
   ],
   "source": [
    "split = tokenized.train_test_split(test_size=0.20, seed=42)\n",
    "test_valid = split[\"test\"].train_test_split(test_size=0.50, seed=42)\n",
    "\n",
    "datasets = {\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": test_valid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in datasets.items()})\n",
    "# -> {'train': 4000, 'validation': 500, 'test': 500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc1abd-e827-4a18-96c8-81ac2924c7da",
   "metadata": {},
   "source": [
    "### PART-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3850ed0-b33a-4814-a91c-bf9c0c4e6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4000, 'validation': 500, 'test': 500}\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if needed\n",
    "# pip install datasets transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import PegasusTokenizer\n",
    "\n",
    "# 1. Load & subsample 5 000 examples\n",
    "raw = load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
    "raw = raw.shuffle(seed=42).select(range(5000))\n",
    "\n",
    "# 2. Load Pegasus‐XSum tokenizer and set special tokens\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "# By default PegasusTokenizer.pad_token == '<pad>' and eos_token == '</s>'\n",
    "# If you want to pad with eos instead of a separate pad symbol:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 3. Define max lengths\n",
    "max_input_length  = 512   # Pegasus‐XSum max supported input\n",
    "max_target_length = 256\n",
    "\n",
    "# 4. Tokenization function\n",
    "def tokenize_fn(batch):\n",
    "    # Encode articles\n",
    "    enc = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    # Encode abstracts as labels\n",
    "    dec = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_target_length,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": dec[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "# 5. Apply tokenization and remove original columns\n",
    "tokenized = raw.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=raw.column_names,\n",
    ")\n",
    "\n",
    "# 6. Split into Train/Validation/Test (80/10/10)\n",
    "split_1 = tokenized.train_test_split(test_size=0.20, seed=42)\n",
    "split_2 = split_1[\"test\"].train_test_split(test_size=0.50, seed=42)\n",
    "\n",
    "datasets = {\n",
    "    \"train\": split_1[\"train\"],         # 4 000 samples\n",
    "    \"validation\": split_2[\"train\"],    #   500 samples\n",
    "    \"test\": split_2[\"test\"],           #   500 samples\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in datasets.items()})\n",
    "# -> {'train': 4000, 'validation': 500, 'test': 500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c8155-d8c5-4eb5-bf5d-dbc6ef050de9",
   "metadata": {},
   "source": [
    "### PART-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03614c9a-4ec4-4e88-8f3e-9dff1254fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 15:34:57.762753: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 15:34:57.770256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747337697.779148 1077159 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747337697.781757 1077159 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747337697.788763 1077159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747337697.788770 1077159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747337697.788771 1077159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747337697.788772 1077159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 15:34:57.791181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1077159/90749916.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mostrich-mt2\u001b[0m (\u001b[33mostrich-mt2-fast-nuces\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/taha/wandb/run-20250515_153503-onyzaikc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ostrich-mt2-fast-nuces/huggingface/runs/onyzaikc' target=\"_blank\">./lora-pegasus-xsum</a></strong> to <a href='https://wandb.ai/ostrich-mt2-fast-nuces/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ostrich-mt2-fast-nuces/huggingface' target=\"_blank\">https://wandb.ai/ostrich-mt2-fast-nuces/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ostrich-mt2-fast-nuces/huggingface/runs/onyzaikc' target=\"_blank\">https://wandb.ai/ostrich-mt2-fast-nuces/huggingface/runs/onyzaikc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 48:34, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.579700</td>\n",
       "      <td>2.768593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.933300</td>\n",
       "      <td>2.575604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.777700</td>\n",
       "      <td>2.544457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.742800</td>\n",
       "      <td>2.536259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-xsum\", device_map=\"auto\")\n",
    "\n",
    "# LoRA\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=16,\n",
    "                         lora_dropout=0.1, target_modules=[\"q_proj\",\"v_proj\"])\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "# Args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./lora-pegasus-xsum\",\n",
    "    per_device_train_batch_size=4, per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=False,            # ← disable half-precision\n",
    "    bf16=False,            # ← or try bf16 if supported\n",
    "    learning_rate=3e-5,    # smaller LR also helps stability\n",
    "    max_grad_norm=1.0,     # gradient clipping\n",
    "    warmup_steps=500,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,\n",
    "    generation_num_beams=4,\n",
    ")\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.save_model(\"./lora-pegasus-xsum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0a18c1-1dc6-41f3-8825-9de023041943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Reload & split raw test set (to restore article/abstract)\n",
    "raw = (\n",
    "    load_dataset(\"ccdv/arxiv-summarization\", split=\"train\")\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(5000))\n",
    ")\n",
    "split1 = raw.train_test_split(test_size=0.20, seed=42)\n",
    "split2 = split1[\"test\"].train_test_split(test_size=0.50, seed=42)\n",
    "raw_test = split2[\"test\"]  # has 'article' & 'abstract'\n",
    "\n",
    "# 2. Reattach text columns to your tokenized-only test split\n",
    "#    (assumes `datasets[\"test\"]` exists from your earlier tokenization)\n",
    "tokenized_test = (\n",
    "    datasets[\"test\"]\n",
    "    .add_column(\"article\", raw_test[\"article\"])\n",
    "    .add_column(\"abstract\", raw_test[\"abstract\"])\n",
    ")\n",
    "\n",
    "# 3. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "# 4. Load two separate model instances:\n",
    "#    A. Base Pegasus\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/pegasus-xsum\", device_map=\"auto\"\n",
    ")\n",
    "#    B. Fine-tuned (wraps a fresh base internally)\n",
    "ft_base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/pegasus-xsum\", device_map=\"auto\"\n",
    ")\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    ft_base_model, \"./lora-pegasus-xsum\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 5. Summarization helper\n",
    "def generate_summaries(model, articles):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for art in articles:\n",
    "        inputs = tokenizer(\n",
    "            art,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        sum_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        outs.append(tokenizer.decode(sum_ids[0], skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "# 6. Select 10 samples and run inference\n",
    "articles      = tokenized_test[\"article\"][:10]\n",
    "ground_truths = tokenized_test[\"abstract\"][:10]\n",
    "\n",
    "base_summaries      = generate_summaries(base_model,      articles)\n",
    "finetuned_summaries = generate_summaries(finetuned_model, articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b39311b-d764-481d-b096-268146426467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE: the performance guarantee of local search and greedy algorithms for scheduling problems is well studied and understood . for most algorithms , matching upper and lower bounds on their approximation ra …\n",
      "BASE:      We study the robustness of local search and greedy algorithms for scheduling problems .\n",
      "FINE-TUNED: we study the performance guarantee of local search and greedy algorithms for scheduling problems in the framework of smoothed analysis , in which instances are subject to some degree of random noise . since pure nash equilibria can be seen as local optima , our results also imply a new bound on the smoothed price of anarchy , showing that known worst - case results are too pessimistic in the presence of noise . since pure nash equilibria can be seen as local optima , our results also imply a new bound on the smoothed price of anarchy , showing that known worst - case results are too pessimistic in the presence of noise .\n",
      "--------------------------------------------------------------------------------\n",
      "ARTICLE: the dynamics of an atomic wave packet in a periodic potential under the influence of a static force has been extensively analyzed using different physical approaches : in terms of wannier - stark reso …\n",
      "BASE:      In our group we have developed a new approach to the detection of falling atomic pulses in optical lattices .\n",
      "FINE-TUNED: we report the detection of the acceleration of gravity @xmath0 in the falling atomic cloud . the detection is performed by imaging the falling atomic cloud using absorption imaging techniques and the uncertainty on the @xmath0 determination did not exceed @xmath7 dominated by the imaging system . in both experiments the detection is performed by imaging the falling atomic cloud using absorption imaging techniques and the uncertainty on the @xmath0 determination did not exceed @xmath7 dominated by the imaging system . our experimental approach is based on the precise determination of the velocity distribution of atoms along the vertical axis using doppler - sensitive raman transitions .\n",
      "--------------------------------------------------------------------------------\n",
      "ARTICLE: transport in a two dimensional electron gas is affected by the spin - orbit coupling . \n",
      " the most common signature is the weak ( anti)localization . \n",
      " it is a small correction to the conductance due t …\n",
      "BASE:      The spin - orbit coupling of a two dimensional electron gas is affected by the presence or absence of a time reversal symmetry breaking magnetic field.\n",
      "FINE-TUNED: we study the dependence of the weak correction to the spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field and spin - orbit coupling dependence of the average conductance on spin - orbit coupling dependence of the magnetic field .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ARTICLE:\", articles[i][:200], \"…\")\n",
    "    print(\"BASE:     \", base_summaries[i])\n",
    "    print(\"FINE-TUNED:\", finetuned_summaries[i])\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb017bf-a0fc-4430-89dc-04453bbce71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d11dc5bd-2c76-4d5f-b3bd-934f7c63aa78",
   "metadata": {},
   "source": [
    "### PART-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e31069-af84-4fb3-b665-2afb1171eddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472dbd8-ba3e-4f33-84eb-1b2bf8be390d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b8685e8-5ebe-4501-8c2f-57363f2daf3e",
   "metadata": {},
   "source": [
    "### PART-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28caab57-b634-4dfc-8dd3-f44a99140d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a19fa-c885-47e8-b80b-3a05b994ada7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b62db19-ba1c-4054-9723-07ffa2b48606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Part 4: Evaluation ===\n",
    "\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load metrics\n",
    "rouge_metric     = evaluate.load(\"rouge\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "cc = SmoothingFunction().method4\n",
    "\n",
    "# 2. Compute all four metrics for a given set of summaries\n",
    "def compute_all_metrics(hyps, refs):\n",
    "    # ROUGE\n",
    "    rouge_res = rouge_metric.compute(\n",
    "        predictions=hyps, references=refs, use_stemmer=True\n",
    "    )\n",
    "    avg_rouge1 = float(rouge_res[\"rouge1\"])\n",
    "    avg_rougeL = float(rouge_res[\"rougeL\"])\n",
    "\n",
    "    # BLEU via NLTK\n",
    "    bleu_scores = [\n",
    "        sentence_bleu([r.split()], h.split(), smoothing_function=cc)\n",
    "        for h, r in zip(hyps, refs)\n",
    "    ]\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    # BERTScore F1\n",
    "    bert_res = bertscore_metric.compute(\n",
    "        predictions=hyps,\n",
    "        references=refs,\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    avg_bertscore = sum(bert_res[\"f1\"]) / len(bert_res[\"f1\"])\n",
    "\n",
    "    return {\n",
    "        \"ROUGE-1\":      avg_rouge1,\n",
    "        \"ROUGE-L\":      avg_rougeL,\n",
    "        \"BLEU\":         avg_bleu,\n",
    "        \"BERTScore F1\": avg_bertscore,\n",
    "    }\n",
    "\n",
    "# 3. Score both models\n",
    "base_scores = compute_all_metrics(base_summaries,      ground_truths)\n",
    "ft_scores   = compute_all_metrics(finetuned_summaries, ground_truths)\n",
    "\n",
    "# 4. Build DataFrame for plotting\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\":       list(base_scores.keys()),\n",
    "    \"Base\":         list(base_scores.values()),\n",
    "    \"Fine-tuned\":   list(ft_scores.values()),\n",
    "})\n",
    "\n",
    "# 5. Plot side-by-side grouped bar chart\n",
    "x = range(len(df))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar([i - 0.2 for i in x], df[\"Base\"],       width=0.4, label=\"Base\")\n",
    "plt.bar([i + 0.2 for i in x], df[\"Fine-tuned\"], width=0.4, label=\"Fine-tuned\")\n",
    "plt.xticks(x, df[\"Metric\"])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Base vs. Fine-tuned Pegasus-XSum Evaluation (10 Samples)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
